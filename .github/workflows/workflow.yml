# .github/workflows/ci_cd.yml
name: CI/CD and Scheduled Retraining Pipeline with DVC

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 */6 * * *' # Runs every 6 hours
  workflow_dispatch:

env:
  DOCKER_IMAGE_NAME: california-housing-api
  DOCKER_REGISTRY: docker.io
  PYTHON_VERSION: "3.9"
  DVC_CACHE_DIR: "/tmp/dvc-cache"

jobs:
  # =================================================================
  # === SETUP AND CACHE JOB (SHARED BY MULTIPLE JOBS) =============
  # =================================================================
  setup:
    if: github.event_name != 'schedule'
    runs-on: ubuntu-latest
    outputs:
      cache-hit: ${{ steps.cache-deps.outputs.cache-hit }}
      python-cache-key: ${{ steps.cache-key.outputs.key }}
      dvc-cache-key: ${{ steps.dvc-cache-key.outputs.key }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Generate cache keys
        id: cache-key
        run: |
          echo "key=${{ runner.os }}-python-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/requirements.txt') }}" >> $GITHUB_OUTPUT
      
      - name: Generate DVC cache key
        id: dvc-cache-key
        run: |
          # Create a cache key based on DVC files and data dependencies
          HASH=$(find . -name "*.dvc" -o -name "dvc.yaml" -o -name "dvc.lock" | sort | xargs cat | sha256sum | cut -d' ' -f1 || echo "no-dvc-files")
          echo "key=dvc-cache-v2-${{ runner.os }}-${HASH}" >> $GITHUB_OUTPUT
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Cache pip dependencies
        id: cache-deps
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ steps.cache-key.outputs.key }}
          restore-keys: |
            ${{ runner.os }}-python-${{ env.PYTHON_VERSION }}-
      
      - name: Install dependencies
        if: steps.cache-deps.outputs.cache-hit != 'true'
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-cov flake8 black dvc

  # =================================================================
  # === DVC SETUP WITH LOCAL STORAGE ===============================
  # =================================================================
  dvc-setup:
    if: github.event_name != 'schedule'
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for DVC
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Restore pip cache
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup.outputs.python-cache-key }}
          restore-keys: |
            ${{ runner.os }}-python-${{ env.PYTHON_VERSION }}-
      
      - name: Install DVC
        run: |
          python -m pip install --upgrade pip
          pip install dvc
      
      - name: Setup DVC cache directory
        run: |
          mkdir -p ${{ env.DVC_CACHE_DIR }}
          echo "DVC_CACHE_DIR=${{ env.DVC_CACHE_DIR }}" >> $GITHUB_ENV
      
      - name: Cache DVC files and artifacts
        uses: actions/cache@v3
        with:
          path: |
            ${{ env.DVC_CACHE_DIR }}
            .dvc/cache
            data/
            models/
          key: ${{ needs.setup.outputs.dvc-cache-key }}
          restore-keys: |
            dvc-cache-v2-${{ runner.os }}-
      
      - name: Initialize DVC with local storage
        run: |
          # Initialize DVC if not already done
          if [ ! -f ".dvc/config" ]; then
            dvc init --no-scm
            # Set up local remote storage in a dedicated directory
            mkdir -p dvc-storage
            dvc remote add -d local-storage ./dvc-storage
            dvc remote modify local-storage type local
          fi
          
          # Configure DVC to use our cache directory
          dvc config cache.dir ${{ env.DVC_CACHE_DIR }}
      
      - name: Generate sample data if not exists
        run: |
          # Create initial data structure if it doesn't exist
          mkdir -p data/raw data/processed models reports logs
          
          # Generate sample data for first run (if needed)
          if [ ! -f "data/raw/sample_data.csv" ] && [ ! -f "data/raw.dvc" ]; then
            python -c "
            import pandas as pd
            import numpy as np
            from sklearn.datasets import fetch_california_housing
            
            # Generate sample data
            housing = fetch_california_housing()
            df = pd.DataFrame(housing.data, columns=housing.feature_names)
            df['target'] = housing.target
            
            # Save to data directory
            df.to_csv('data/raw/california_housing.csv', index=False)
            print('Sample data generated')
            "
          fi
      
      - name: Add data to DVC tracking
        run: |
          # Track data with DVC if not already tracked
          if [ -d "data/raw" ] && [ ! -f "data/raw.dvc" ]; then
            dvc add data/raw
            echo "Added data/raw to DVC tracking"
          fi
          
          # Try to pull from local remote (will fail gracefully on first run)
          dvc pull || echo "No DVC remote data to pull (first run)"

  # =================================================================
  # === PARALLEL TESTING AND QUALITY CHECKS ========================
  # =================================================================
  test-and-lint:
    if: github.event_name != 'schedule'
    runs-on: ubuntu-latest
    needs: [setup, dvc-setup]
    strategy:
      fail-fast: false
      matrix:
        task: [lint, test, security]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Restore pip cache
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup.outputs.python-cache-key }}
      
      - name: Restore DVC cache
        uses: actions/cache@v3
        with:
          path: |
            ${{ env.DVC_CACHE_DIR }}
            .dvc/cache
            data/
            models/
          key: ${{ needs.setup.outputs.dvc-cache-key }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-cov flake8 black dvc
      
      - name: Setup DVC environment
        run: |
          mkdir -p ${{ env.DVC_CACHE_DIR }}
          dvc config cache.dir ${{ env.DVC_CACHE_DIR }}
      
      - name: Create necessary directories
        run: mkdir -p data/processed models logs reports
      
      # Conditional steps based on matrix task
      - name: Lint with flake8 and black
        if: matrix.task == 'lint'
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
          black --check . || echo "Code formatting issues found. Run 'black .' to fix."
      
      - name: Generate test data
        if: matrix.task == 'test'
        run: |
          # Pull DVC data first
          dvc pull || echo "No DVC data to pull"
          python src/data_preprocessing.py
      
      - name: Run tests with pytest
        if: matrix.task == 'test'
        run: |
          pytest tests/ -v --cov=src --cov-report=xml --cov-report=html
      
      - name: Upload coverage to Codecov
        if: matrix.task == 'test'
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
      
      - name: Run Trivy vulnerability scanner
        if: matrix.task == 'security'
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy scan results to GitHub Security tab
        if: matrix.task == 'security'
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'

  # =================================================================
  # === STREAMLINED ML PIPELINE WITH DVC ===========================
  # =================================================================
  ml-pipeline:
    if: github.event_name != 'schedule'
    runs-on: ubuntu-latest
    needs: [test-and-lint, dvc-setup]
    
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT || github.token }}
      
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Restore pip cache
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ needs.setup.outputs.python-cache-key }}
      
      - name: Restore DVC cache
        uses: actions/cache@v3
        with:
          path: |
            ${{ env.DVC_CACHE_DIR }}
            .dvc/cache
            data/
            models/
          key: ${{ needs.setup.outputs.dvc-cache-key }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc
      
      - name: Setup DVC environment
        run: |
          mkdir -p ${{ env.DVC_CACHE_DIR }}
          dvc config cache.dir ${{ env.DVC_CACHE_DIR }}
      
      - name: Pull DVC data
        run: |
          dvc pull || echo "No DVC data to pull (might be first run)"
      
      - name: Run complete ML pipeline
        run: |
          # Data preprocessing
          echo "ðŸ”„ Starting data preprocessing..."
          python src/data_preprocessing.py
          
          # Model training
          echo "ðŸ¤– Starting model training..."
          python scripts/train_models.py
          
          # Model validation
          echo "âœ… Validating model performance..."
          python scripts/validate_model.py
          
          # Model drift check
          echo "ðŸ“Š Checking model drift..."
          python scripts/check_model_drift.py
          
          # Update monitoring metrics
          echo "ðŸ“ˆ Updating monitoring metrics..."
          python src/monitoring.py
      
      - name: Initialize A/B testing framework
        run: |
          mkdir -p logs
          python -c "
          from src.ab_testing import ABTestManager
          from datetime import datetime, timedelta
          
          test_manager = ABTestManager()
          print('A/B testing database initialized')
          
          start_date = datetime.now().isoformat()
          end_date = (datetime.now() + timedelta(days=30)).isoformat()
          
          test_manager.create_test(
              test_name='model_comparison',
              variants={'control': 0.8, 'challenger': 0.2},
              start_date=start_date,
              end_date=end_date
          )
          print('Model comparison A/B test created: 80% control, 20% challenger')
          "
      
      - name: Update DVC tracking and push
        run: |
          # Track processed data and models with DVC
          dvc add data/processed/ || echo "Data already tracked or no changes"
          dvc add models/ || echo "Models already tracked or no changes" 
          
          # Push to local DVC remote
          dvc push || echo "No new data to push to DVC remote"
      
      - name: Commit DVC metadata changes
        run: |
          # Configure git for commits
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          # Add DVC files and any new gitignore entries
          git add -A *.dvc dvc.lock .gitignore 2>/dev/null || echo "No DVC files to add"
          
          # Also add model metadata that should be tracked in git
          git add models/model_metadata.json 2>/dev/null || echo "No model metadata to add"
          
          if ! git diff --staged --quiet; then
            git commit -m "feat: update ML pipeline artifacts and DVC tracking"
            git push
            echo "âœ… DVC metadata and model artifacts committed to repository."
          else
            echo "â„¹ï¸  No DVC changes to commit."
          fi

  # =================================================================
  # === OPTIMIZED BUILD AND DEPLOY =================================
  # =================================================================
  build-and-deploy:
    if: github.event_name != 'schedule' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    needs: ml-pipeline
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64
      
      - name: Deploy to production
        run: |
          echo "ðŸš€ Deploying to production environment..."
          chmod +x scripts/deploy.sh
          ./scripts/deploy.sh production
        env:
          DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
          DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
          DOCKER_REGISTRY: ${{ env.DOCKER_REGISTRY }}
          DOCKER_IMAGE_NAME: ${{ env.DOCKER_IMAGE_NAME }}
      
      - name: Run integration tests
        run: |
          echo "ðŸ§ª Running integration tests..."
          sleep 30
          python tests/integration_tests.py

  # =================================================================
  # === AUTOMATED RETRAINING WITH DVC (SCHEDULED) ==================
  # =================================================================
  auto-retrain-and-commit:
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT || github.token }}
          fetch-depth: 0

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-python-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-${{ env.PYTHON_VERSION }}-

      - name: Cache DVC artifacts for retraining
        uses: actions/cache@v3
        with:
          path: |
            ${{ env.DVC_CACHE_DIR }}
            .dvc/cache
            data/
            models/
          key: dvc-retrain-${{ github.run_id }}
          restore-keys: |
            dvc-cache-v2-${{ runner.os }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc

      - name: Setup DVC environment for retraining
        run: |
          mkdir -p ${{ env.DVC_CACHE_DIR }}
          dvc config cache.dir ${{ env.DVC_CACHE_DIR }}

      - name: Pull latest data and models from DVC
        run: |
          dvc pull || echo "No DVC files to pull"

      - name: Check for retraining and execute
        id: retrain_check
        run: |
          python -c "
          from scripts.auto_train import AutoRetrainer
          import os
          
          retrainer = AutoRetrainer()
          if retrainer.check_retrain_conditions():
              print('ðŸ”„ Retraining conditions met. Starting model retraining.')
              retrainer.retrain_model()
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write('retrained=true\n')
          else:
              print('â„¹ï¸  Retraining conditions not met. No action needed.')
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write('retrained=false\n')
          "

      - name: Update DVC tracking and commit retrained models
        if: steps.retrain_check.outputs.retrained == 'true'
        run: |
          # Add updated models to DVC tracking
          dvc add models/ || echo "Models already tracked"
          
          # Push to DVC remote storage
          dvc push || echo "No changes to push to DVC remote"
          
          # Configure git
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          # Add DVC metadata files and model metadata to git
          git add models.dvc models/model_metadata.json .gitignore dvc.lock 2>/dev/null || echo "No DVC files to add"
          
          if ! git diff --staged --quiet; then
            git commit -m "chore(bot): automated model retraining and DVC update

            ðŸ¤– Automated retraining completed
            ðŸ“Š Model performance metrics updated  
            ðŸ”„ DVC tracking updated for new model artifacts
            â° Triggered by scheduled workflow"
            
            git push
            echo "âœ… Retrained model committed to repository via DVC."
          else
            echo "â„¹ï¸  Model was retrained, but no changes detected in DVC metadata."
          fi

      - name: Create retraining summary
        if: steps.retrain_check.outputs.retrained == 'true'
        run: |
          echo "## ðŸ¤– Automated Retraining Summary" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Model retraining completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“Š New model artifacts stored in DVC" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”„ Repository updated with new model metadata" >> $GITHUB_STEP_SUMMARY
          echo "- â° Completed at: $(date)" >> $GITHUB_STEP_SUMMARY
          
          # Add model performance info if available
          if [ -f "models/model_metadata.json" ]; then
            echo "- ðŸ“ˆ Model Performance:" >> $GITHUB_STEP_SUMMARY
            python -c "
            import json
            with open('models/model_metadata.json', 'r') as f:
                metadata = json.load(f)
            print(f\"  - RÂ² Score: {metadata.get('metrics', {}).get('r2', 'N/A')}\")
            print(f\"  - Model Type: {metadata.get('best_model', 'N/A')}\")
            " >> $GITHUB_STEP_SUMMARY
          fi