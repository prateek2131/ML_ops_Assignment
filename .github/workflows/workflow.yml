# .github/workflows/ci_cd.yml
name: CI/CD and Scheduled Retraining Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  # ADDED: Triggers for the new retraining job
  schedule:
    - cron: '*/15 * * * *' # Runs every 6 hours
  workflow_dispatch:


env:
  DOCKER_IMAGE_NAME: california-housing-api
  DOCKER_REGISTRY: docker.io

jobs:
  # IMPORTANT: A condition is added to every existing job
  # to PREVENT it from running on the schedule.
  setup-and-test:
    if: github.event_name != 'schedule' # <-- ADDED
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9]

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-cov flake8 black
    
    - name: Create necessary directories
      run: |
        mkdir -p data/processed models logs
    
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Generate test data
      run: |
        python src/data_preprocessing.py
    
    - name: Run tests with pytest
      run: |
        pytest tests/ -v --cov=src --cov-report=xml --cov-report=html
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  security-scan:
    if: github.event_name != 'schedule' # <-- ADDED
    runs-on: ubuntu-latest
    needs: setup-and-test
    permissions:
      actions: write
      contents: read
      security-events: write
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  data-preprocessing:
    if: github.event_name != 'schedule' # <-- ADDED
    runs-on: ubuntu-latest
    needs: setup-and-test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Data Preprocessing Training
      run: |
        python src/data_preprocessing.py

  model-training:
    if: github.event_name != 'schedule' # <-- ADDED
    runs-on: ubuntu-latest
    needs: data-preprocessing
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Data Preprocessing Training
      run: |
        python src/data_preprocessing.py
        
    - name: Model Training
      run: |
        python scripts/train_models.py

  model-performance-validation:
    if: github.event_name != 'schedule' # <-- ADDED
    runs-on: ubuntu-latest
    needs: model-training
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Data Preprocessing Training
      run: |
        python src/data_preprocessing.py
        
    - name: Model Training
      run: |
        python scripts/train_models.py

    - name: Validate model performance
      run: |
        python scripts/validate_model.py

  model-drift-check:
    if: github.event_name != 'schedule' # <-- ADDED
    runs-on: ubuntu-latest
    needs: model-performance-validation
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Check model drift
      run: |
        python scripts/check_model_drift.py
    
    - name: Update monitoring metrics
      run: |
        python src/monitoring.py

  monitoring-health-check:
    if: github.event_name != 'schedule' # <-- ADDED
    runs-on: ubuntu-latest
    needs: model-drift-check
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install prometheus-client psutil
    
    - name: Run monitoring health check
      run: |
        python -c "
        from src.monitoring import ModelMonitor
        import time
        monitor = ModelMonitor()
        print('Monitoring system initialized successfully')
        time.sleep(5)
        print('Health check completed')
        "
    
    - name: Validate monitoring data
      run: |
        python -c "
        import os
        import json
        if os.path.exists('models/model_metadata.json'):
            with open('models/model_metadata.json', 'r') as f:
                metadata = json.load(f)
            print(f'Model accuracy: {metadata[\"metrics\"][\"r2\"]}')
            print(f'Active model: {metadata[\"best_model\"]}')
            print('Monitoring validation passed')
        else:
            print('Warning: model_metadata.json not found')
        "

  ab-testing-setup:
    if: github.event_name != 'schedule' # <-- ADDED
    runs-on: ubuntu-latest
    needs: monitoring-health-check
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create logs directory
      run: |
        mkdir -p logs
    
    - name: Initialize A/B testing framework
      run: |
        python -c "
        from src.ab_testing import ABTestManager
        import os
        from datetime import datetime, timedelta
        
        test_manager = ABTestManager()
        print('A/B testing database initialized')
        
        start_date = datetime.now().isoformat()
        end_date = (datetime.now() + timedelta(days=30)).isoformat()
        
        test_manager.create_test(
            test_name='model_comparison',
            variants={'control': 0.8, 'challenger': 0.2},
            start_date=start_date,
            end_date=end_date
        )
        print('Model comparison A/B test created: 80% control, 20% challenger')
        
        for i in range(10):
            variant = test_manager.get_variant('model_comparison', f'user_{i}')
            print(f'User {i} assigned to variant: {variant}')
        "
    
    - name: Validate A/B testing setup
      run: |
        python -c "
        from src.ab_testing import ABTestManager, get_model_variant
        import os
        
        if not os.path.exists('logs/ab_tests.db'):
            raise Exception('A/B testing database not found')
        print('A/B testing database created successfully')
        
        test_manager = ABTestManager()
        model, variant = get_model_variant(test_manager, 'test_user')
        print(f'Test user gets model: {model}, variant: {variant}')
        
        test_manager.log_test_result(
            test_name='model_comparison',
            user_id='test_user',
            variant=variant,
            input_data={'test': 'data'},
            prediction=0.85,
            response_time=0.2
        )
        print('A/B test result logged successfully')
        
        results = test_manager.get_test_results('model_comparison')
        print(f'A/B test results: {results}')
        print('A/B testing validation completed')
        "

  build-and-push:
    if: github.event_name != 'schedule' && github.ref == 'refs/heads/main' # <-- MODIFIED
    runs-on: ubuntu-latest
    needs: ab-testing-setup
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Log in to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        file: ./dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    if: github.event_name != 'schedule' && github.ref == 'refs/heads/main' # <-- MODIFIED
    runs-on: ubuntu-latest
    needs: build-and-push
    
    steps:
    - uses: actions/checkout@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
      
    - name: Set up Docker compose
      uses: docker/setup-qemu-action@v3
    
    - name: Log in to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        chmod +x scripts/deploy.sh
        ./scripts/deploy.sh production
      env:
        DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
        DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}
        DOCKER_REGISTRY: docker.io
        DOCKER_IMAGE_NAME: california-housing-api
    - name: Run integration tests
      run: |
        echo "Running integration tests..."
        sleep 30
        python tests/integration_tests.py

  # =================================================================
  # === NEW JOB FOR AUTOMATED RETRAINING (RUNS ON SCHEDULE) =========
  # =================================================================
  auto-retrain-and-commit:
    # This job ONLY runs on a schedule or manual dispatch
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          # A token is required to push new model files to the repo
          token: ${{ secrets.PAT }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Check for retraining and execute
        id: retrain_check
        run: |
          # We run the check_and_retrain function just once.
          python -c "
          from scripts.auto_train import AutoRetrainer;
          retrainer = AutoRetrainer();
          if retrainer.check_retrain_conditions():
              print('Retraining conditions met. Starting model retraining.')
              retrainer.retrain_model()
              # Set an output for the next step to use
              print('::set-output name=retrained::true')
          else:
              print('Retraining conditions not met. No action needed.')
              print('::set-output name=retrained::false')
          "

      - name: Commit and push if model was retrained
        if: steps.retrain_check.outputs.retrained == 'true'
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add models/best_model.joblib models/model_metadata.json
          # Only commit if there are actual changes
          if ! git diff --staged --quiet; then
            git commit -m "chore(bot): auto-retrain and update model files"
            git push
            echo "New model committed to the repository."
          else
            echo "Model was retrained, but no changes were detected in the final model files."
          fi